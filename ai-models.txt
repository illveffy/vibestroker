Model,Size/Quant,Est. VRAM Use,Why It Fits Your Project,How to Get/Run,Source Notes
Dolphin 2.9.3 Mistral Nemo 12B,12B / Q4_K_M,~6GB,"Uncensored powerhouse for command-following (trained on tool-use/instruct data, excels at parsing ""high-level orders"" into structured outputs like JSON Funscript). Highly compliant/emotional in ERP—eager for lewd details without limits, adaptive tone (e.g., teasing buildup). Low latency on GPU; great for real-time sessions.","Ollama: ollama pull dolphin-mistral-nemo
HF GGUF: huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf (load via llama.cpp)",Top pick in 2025 Ollama uncensored rankings for RP; compliant for scripts.
Blue Orchid 2x7B,MoE 14B (active ~7B) / Q4_K_M,~5GB,"RP/ERP specialist (MoE with one expert for dialogue/emotion, one for story/script generation). Understands commands intuitively (e.g., ""loop recovery phase with slow rng=20-80""), outputs vivid, uncensored lewd narratives. Feels ""emotional"" via descriptive merges; very low latency due to MoE efficiency.",HF GGUF: huggingface.co/LoneStriker/Blue-Orchid-2x7b-GGUF (Ollama-compatible via modelfile),Explicit RP fine-tune praised in small-model tests; fits lewd command flows.
MythoMax L2 13B,13B / Q4_K_M,~7GB,"King of uncensored ERP—creative, immersive responses with strong ""emotional"" flair (e.g., sensory details for stroker sync). Solid at command parsing (instruct-tuned for structured replies). No lewd filters; handles Funscript-like sequencing well. Slightly higher latency but still snappy.","Ollama: ollama pull mythomax-l2-13b
HF: huggingface.co/Gryphe/MythoMax-L2-13b",2025 RP benchmark winner for uncensored depth; ideal for interactive lewd chats.

currently using Dolphin 2.9.3 Mistral Nemo 12B. uses around 8gb of vram. running on LM Studio